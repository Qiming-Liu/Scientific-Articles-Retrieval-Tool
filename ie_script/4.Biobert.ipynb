{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d9d549b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d5e1a4",
   "metadata": {},
   "source": [
    "file cleaned_data.sav from step 1.Pre.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d49ae7c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "in_file  = \"./cleaned_data.sav\"\n",
    "df = pd.read_pickle(open(in_file, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "109cb46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [10:32<00:00, 63.22s/it]\n"
     ]
    }
   ],
   "source": [
    "df_split = np.array_split(df, 10)\n",
    "for i in tqdm(range(len(df_split))):\n",
    "    body_text = []\n",
    "    for article in df_split[i]['body_text']:\n",
    "        body_text.append(article)\n",
    "    pickle.dump(body_text, open('./body_text/body_text_'+str(i)+'.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff38463",
   "metadata": {},
   "source": [
    "sav to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85f66e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import json\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "nlp.max_length = 2000000\n",
    "\n",
    "def to_sent(text):\n",
    "    doc=nlp(text)\n",
    "    result=[]\n",
    "    for sent in list(doc.sents):\n",
    "        sent=str(sent)\n",
    "        sent = sent.rstrip() #remove end space\n",
    "        result.append(sent)\n",
    "    return result\n",
    "\n",
    "def to_json(df, save_file_name):\n",
    "    results = []\n",
    "    \n",
    "    for text in tqdm(df):\n",
    "        results += to_sent(text)\n",
    "    \n",
    "    json_file = save_file_name + '.json'\n",
    "    with open(json_file,'w') as f:\n",
    "        json.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b6530b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# abstract\n",
    "abstract = df['abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaa3af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_json(abstract, 'abstract')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d61296a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# body_text\n",
    "filename = '0' # 1 2 3 4 ... 9\n",
    "df = pd.read_pickle(open('./body_text/body_text_'+filename+'.sav', 'rb'))\n",
    "to_json(df, './body_text/body_text_'+filename+'.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a0f35c",
   "metadata": {},
   "source": [
    "minie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "42df0cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in file minie_bash.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c8d01a",
   "metadata": {},
   "source": [
    "filter and label sentence for biobert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "531f28d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pronounsList = [\"another\", \"anybody\", \"anyone\", \"anything\", \"each\", \"either\", \"enough\", \"everybody\", \"everyone\",\n",
    "                \"everything\", \"little\", \"much\", \"neither\", \"nobody\", \"no one\", \"nothing\", \"one\", \"other\", \"somebody\",\n",
    "                \"something\", \"both\", \"few\", \"fewer\", \"many\", \"others\", \"several\", \"all\", \"any\", \"more\", \"most\",\n",
    "                \"someone\", \"none\", \"some\", \"such\", \"I\", \"me\", \"my\", \"mine\", \"you\", \"your\", \"yours\", \"yourself\",\n",
    "                \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"myself\", \"it\", \"its\", \n",
    "                \"itself\", \"we\", \"us\", \"our\", \"ours\", \"ourselves\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\"]\n",
    "\n",
    "def label(csv_file, name):\n",
    "    csv_data = pd.read_csv(csv_file,encoding='utf-8')\n",
    "    csv_df = pd.DataFrame(csv_data)\n",
    "    \n",
    "    # remove QUANT_\n",
    "    df = csv_df.dropna()[~csv_df.dropna()['subject'].str.contains('QUANT_')]\n",
    "    df = df.dropna()[~df.dropna()['object'].str.contains('QUANT_')]\n",
    "    \n",
    "    # remove single pronoun\n",
    "    df = df.dropna()[~df.dropna()['subject'].str.lower().isin(pronounsList)]\n",
    "    df = df.dropna()[~df.dropna()['object'].str.lower().isin(pronounsList)]\n",
    "    \n",
    "    # label sent\n",
    "    def label1(sub,obj,sent):\n",
    "        sub_location = sent.find(sub)\n",
    "        if(sub_location == -1):\n",
    "            return sent\n",
    "        new_sub = '@' + sub.upper() + '#'\n",
    "        sent = sent[:sub_location] + new_sub + sent[sub_location+len(sub):]\n",
    "        obj_location = sent.find(obj)\n",
    "        if(obj_location == -1):\n",
    "            return sent\n",
    "        new_obj = '@' + obj.upper() + '#'\n",
    "        sent = sent[:obj_location] + new_obj + sent[obj_location+len(obj):]\n",
    "        return sent\n",
    "    df['sent'] = [ label1(a,b,c) for a,b,c in zip(df['subject'],df['object'],df['sentence']) ]\n",
    "    \n",
    "    # set label\n",
    "    def label2(a):\n",
    "        if(a[0].count('@') >= 2 and a[0].count('#') >= 2):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    df['label'] = [ label2(a) for a in zip(df['sent']) ]\n",
    "    \n",
    "    # save all\n",
    "    df.insert(0, 'index', range(len(df)), allow_duplicates=False)\n",
    "    df.to_csv('./for_biobert/'+ 'labeled_'+csv_file , index=False)\n",
    "    \n",
    "    # save for test.tsv\n",
    "    df=df.drop('subject',axis=1)\n",
    "    df=df.drop('relation',axis=1)\n",
    "    df=df.drop('object',axis=1)\n",
    "    df=df.drop('sentence',axis=1)\n",
    "    df.columns = ['index','sentence','label']\n",
    "    \n",
    "    df.to_csv('./for_biobert/'+ name + '_test.tsv', sep=\"\\t\" , index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "61340e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "label('abstract.json.csv', 'abstract')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6012db5",
   "metadata": {},
   "source": [
    "run biobert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "65ce7c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in file ./biobert/run_test.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16ac25a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
